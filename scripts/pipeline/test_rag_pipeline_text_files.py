import unittest
import tempfile
import os
import yaml
import pandas as pd
from pathlib import Path
import shutil
from unittest.mock import patch, MagicMock, mock_open

import sys # Must be at the top for sys.modules manipulation
from unittest.mock import MagicMock, patch, mock_open # patch must also be early if used at module level

# --- Global Mocks for Heavy/Problematic Imports ---
# Mock win32com (for EmailFetcher, if imported via RAGPipeline)
try:
    import win32com.client
except ImportError:
    win32com_mock = MagicMock()
    win32com_mock.client = MagicMock()
    win32com_mock.client.Dispatch = MagicMock()
    sys.modules['win32com'] = win32com_mock
    sys.modules['win32com.client'] = win32com_mock.client

# Mock spacy (for TextChunker)
try:
    import spacy
except ImportError:
    spacy_mock = MagicMock()
    # Mock spacy.load() to return a dummy nlp object
    dummy_nlp = MagicMock()
    dummy_nlp.tokenizer = MagicMock() # if tokenizer is used directly
    spacy_mock.load = MagicMock(return_value=dummy_nlp)
    sys.modules['spacy'] = spacy_mock

# Mock transformers (for TextChunker)
try:
    import transformers
except ImportError:
    transformers_mock = MagicMock()
    transformers_mock.AutoTokenizer = MagicMock()
    transformers_mock.AutoModel = MagicMock()
    sys.modules['transformers'] = transformers_mock

# Mock torch (potentially imported by sentence-transformers or transformers)
try:
    import torch
except ImportError:
    torch_mock = MagicMock()
    sys.modules['torch'] = torch_mock

# Mock faiss (for ChunkRetriever)
try:
    import faiss
except ImportError:
    faiss_mock = MagicMock()
    # If faiss has specific attributes/methods used at import time or class definition, mock them too.
    # For now, a simple MagicMock should prevent ModuleNotFoundError.
    sys.modules['faiss'] = faiss_mock

# Mock sentence_transformers (for LocalModelEmbedder -> GeneralPurposeEmbedder)
try:
    import sentence_transformers
except ImportError:
    st_mock = MagicMock()
    # Mock the SentenceTransformer class
    mock_sentence_transformer_class = MagicMock()
    # Mock the __init__ of SentenceTransformer to not require a model_name argument or to accept it
    mock_sentence_transformer_class.return_value = MagicMock() # This is the instance
    st_mock.SentenceTransformer = mock_sentence_transformer_class
    sys.modules['sentence_transformers'] = st_mock
# --- End Global Mocks ---

import unittest
import tempfile
import os
import yaml
import pandas as pd
from pathlib import Path
import shutil

from scripts.pipeline.rag_pipeline import RAGPipeline
# Ensure that necessary models for chunking/embedding are available if not mocking them.
# For this test, we'll use real local models as specified in the config.
# UPDATE: Given environment constraints (no space for models), we will mock heavily.

# Mocks for components that load large models
MOCK_CHUNKED_TEXT_DATA = {
    "The quick brown fox jumps over the lazy dog.": ["The quick brown fox", "jumps over the lazy dog."],
    "Pack my box with five dozen liquor jugs.": ["Pack my box with five", "dozen liquor jugs."]
}

def mock_chunk_text(text_input):
    # Convert to string if it's not, as TextChunker expects string input
    text_input_str = str(text_input)
    return MOCK_CHUNKED_TEXT_DATA.get(text_input_str, [text_input_str]) # Default to single chunk if not in mock data

class TestRAGPipelineTextFiles(unittest.TestCase):

    def setUp(self):
        # Main temporary directory for the test run
        self.test_run_dir_obj = tempfile.TemporaryDirectory()
        self.test_run_dir = Path(self.test_run_dir_obj.name)

        # Create subdirectories
        self.input_text_dir = self.test_run_dir / "input_texts"
        self.input_text_dir.mkdir(parents=True, exist_ok=True)

        self.config_dir = self.test_run_dir / "config"
        self.config_dir.mkdir(parents=True, exist_ok=True)

        # This will be the 'base_dir' for TaskPaths, e.g., "runs" in the default TaskPaths.
        # RAGPipeline's configure_task will create task_name subdir inside this.
        self.pipeline_runs_base_dir = self.test_run_dir / "pipeline_runs"
        self.pipeline_runs_base_dir.mkdir(parents=True, exist_ok=True)
        
        self.task_name = "text_integration_task"

        # Sample .txt files
        self.file1_content = "The quick brown fox jumps over the lazy dog."
        self.file2_content = "Pack my box with five dozen liquor jugs."
        with open(self.input_text_dir / "doc1.txt", "w", encoding="utf-8") as f:
            f.write(self.file1_content)
        with open(self.input_text_dir / "doc2.txt", "w", encoding="utf-8") as f:
            f.write(self.file2_content)

        # Create a minimal, valid configuration YAML file
        # Paths in the config will be dynamically generated by RAGPipeline.configure_task
        # but we need to provide the base input and output structure.
        self.config_data = {
            "task_name": self.task_name,
            "embedding": {
                "mode": "local", # Use local model for testing
                "model_name": "sentence-transformers/all-MiniLM-L6-v2", # A common small model
                "embedding_dim": 384, # Dimension for all-MiniLM-L6-v2
                "output_dir": "", # Will be set by configure_task
                "index_filename": "test_faiss.index",
                "metadata_filename": "test_metadata.tsv"
            },
            "chunking": { # Add chunking config as TextChunker needs it
                "max_chunk_size": 100, # Small chunk size for testing
                "overlap": 10,
                "min_chunk_size": 20,
                "language_model": "en_core_web_sm", # spaCy model
                "embedding_model": "sentence-transformers/all-MiniLM-L6-v2" # for similarity in chunker
            },
            "retrieval": {
                "top_k": 1 # Retrieve just one chunk for simplicity
            },
            "generation": { # For generate_answer step
                "model": "mock_gpt", # Mocked model
                "prompt_template": "standard_qa" 
            },
            "prompting": { # Added for generate_answer to pick up style
                "style": "default"
            },
            "text_files": { # Specific to text file processing
                "input_dir": str(self.input_text_dir)
            },
            # paths will be largely populated by RAGPipeline.configure_task
            "paths": {
                 # chunked_emails is in default template, keep it for now or make validation more conditional
                "chunked_emails": "", # Will be set by configure_task
                "chunked_text_files": "", # Will be set by configure_task
                "text_output_dir_raw": "", # Will be set by configure_task
                "email_dir": "", # Will be set by configure_task
                "log_dir": "", # Will be set by configure_task
                "output_dir": "", # Will be set by configure_task (general output, same as embedding.output_dir)
            },
            # Dummy outlook config to pass validation, not used for text files
            "outlook": {
                "account_name": "dummy_account",
                "folder_path": "dummy_folder",
                "days_to_fetch": 0
            },
            "api_clients": { # Added to prevent APIClient init error in ChunkRetriever
                "openai": {
                    "api_key": "DUMMY_API_KEY_FOR_TESTING",
                    "model": "gpt-3.5-turbo" # A default model, though not strictly used if calls are mocked
                }
            }
        }
        
        self.config_file_path = self.config_dir / f"{self.task_name}.yaml"
        with open(self.config_file_path, 'w', encoding='utf-8') as f:
            yaml.dump(self.config_data, f)

        # Initialize RAGPipeline instance and configure it for the task
        # Pass self.pipeline_runs_base_dir as the base for TaskPaths if RAGPipeline's TaskPaths uses a default "runs"
        # The `configure_task` method in RAGPipeline will use this task_name to create subdirs under "configs/tasks"
        # and "runs" (or the TaskPaths base_dir).
        # For testing, we want all outputs within self.test_run_dir.
        # We can achieve this by ensuring RAGPipeline's TaskPaths uses self.pipeline_runs_base_dir.
        # The `configure_task` method saves the final config to `configs/tasks/{task_name}.yaml`
        # For this test, we'll use the config we've created and directly load it.
        # We need to manually adjust the output paths in self.config_data to point within our test_run_dir
        
        # Simulate what configure_task does for paths, but within our controlled test_run_dir
        from scripts.utils.paths import TaskPaths # Import here to avoid top-level if file not found
        
        # We'll use TaskPaths with our custom base_dir for pipeline runs
        tp = TaskPaths(task_name=self.task_name, base_dir=str(self.pipeline_runs_base_dir))

        self.config_data["paths"]["chunked_emails"] = tp.get_chunk_file(data_type="email")
        self.config_data["paths"]["chunked_text_files"] = tp.get_chunk_file(data_type="text_file")
        self.config_data["paths"]["text_output_dir_raw"] = tp.get_raw_text_output_dir()
        self.config_data["paths"]["email_dir"] = tp.emails_dir
        self.config_data["paths"]["log_dir"] = tp.logs_dir
        self.config_data["paths"]["output_dir"] = tp.embeddings_dir # General output dir
        self.config_data["embedding"]["output_dir"] = tp.embeddings_dir # Specific for embeddings

        # Re-save the config with paths correctly pointing into the temp pipeline_runs_base_dir
        with open(self.config_file_path, 'w', encoding='utf-8') as f:
            yaml.dump(self.config_data, f)

        # This pipeline will load the config we just wrote.
        self.pipeline = RAGPipeline(config_path=str(self.config_file_path))


    def tearDown(self):
        # Cleanup the temporary directories
        self.test_run_dir_obj.cleanup()

    @patch('scripts.chunking.text_chunker_v2.TextChunker.__init__', MagicMock(return_value=None))
    @patch('scripts.chunking.text_chunker_v2.TextChunker.chunk_text', side_effect=mock_chunk_text)
    @patch('scripts.embedding.general_purpose_embedder.GeneralPurposeEmbedder.__init__', MagicMock(return_value=None))
    @patch('scripts.embedding.general_purpose_embedder.GeneralPurposeEmbedder.run')
    @patch('scripts.embedding.general_purpose_embedder.GeneralPurposeEmbedder.embed_query')
    @patch('scripts.api_clients.openai.gptApiClient.APIClient.send_completion_request') # Mocks APIClient for generate_answer
    @patch('scripts.retrieval.chunk_retriever_v3.APIClient') # Mocks APIClient specifically for ChunkRetriever
    def test_text_file_processing_workflow(self, 
                                           mock_chunk_retriever_api_client, # Order matches @patch decorators
                                           mock_generate_answer_api_client,
                                           mock_embed_query,
                                           mock_embedder_run,
                                           mock_text_chunker_chunk_text_call): 
        # Configure the mock for APIClient used by ChunkRetriever
        # Prevent ValueError: No API key provided...
        mock_chunk_retriever_api_client.return_value = MagicMock() 
        
        # Mock the LLM call for generate_answer
        mock_generate_answer_api_client.return_value = "This is a mock LLM answer based on text files."
        # If generate_answer's APIClient is used as `APIClient().send_completion_request`
        # then mock_generate_answer_api_client.return_value.send_completion_request.return_value = "..."
        # But RAGPipeline does `client = APIClient(config=self.config); answer = client.send_completion_request(prompt)`
        # So, the mock needs to be on the instance's method.
        # The @patch for APIClient.send_completion_request is more direct for generate_answer.
        # Let's rename mock_generate_answer_api_client to mock_send_completion_request for clarity as it's patching the method.
        # The patch for 'scripts.api_clients.openai.gptApiClient.APIClient.send_completion_request' is actually patching the method directly,
        # not the class. So its argument should reflect that.

        # Correcting mock argument names based on patch order (bottom-up)
        # Original order in decorators (top to bottom):
        # 1. TextChunker.__init__ (not passed to method)
        # 2. TextChunker.chunk_text (mock_text_chunker_chunk_text_call)
        # 3. GPEmbedder.__init__ (not passed to method)
        # 4. GPEmbedder.run (mock_embedder_run)
        # 5. GPEmbedder.embed_query (mock_embed_query)
        # 6. APIClient.send_completion_request (mock_send_completion_request)
        # 7. chunk_retriever_v3.APIClient (mock_chunk_retriever_api_client)

        # The arguments to the test function are based on the order of @patch decorators, from bottom up.
        # So, it should be:
        # mock_chunk_retriever_api_client (from @patch('scripts.retrieval.chunk_retriever_v3.APIClient'))
        # mock_send_completion_request (from @patch('scripts.api_clients.openai.gptApiClient.APIClient.send_completion_request'))
        # mock_embed_query (from @patch('scripts.embedding.general_purpose_embedder.GeneralPurposeEmbedder.embed_query'))
        # mock_embedder_run (from @patch('scripts.embedding.general_purpose_embedder.GeneralPurposeEmbedder.run'))
        # mock_text_chunker_chunk_text_call (from @patch('scripts.chunking.text_chunker_v2.TextChunker.chunk_text'))
        
        # Let's use the names as passed by the decorator order (bottom-up for args):
        # mock_send_completion_request_for_generate_answer
        # mock_api_client_for_chunk_retriever

        # Renaming for clarity based on the actual patch targets and order:
        # Last decorator is @patch('scripts.api_clients.openai.gptApiClient.APIClient.send_completion_request')
        # -> this will be the first arg to the test method if we list them.
        # Next to last is @patch('scripts.retrieval.chunk_retriever_v3.APIClient')
        # -> this will be the second arg.
        # This is confusing. Let's stick to the order in the current signature and adjust the meaning.
        # Current signature:
        # mock_send_completion_request -> from @patch('scripts.api_clients.openai.gptApiClient.APIClient.send_completion_request')
        # mock_embed_query -> from @patch('scripts.embedding.general_purpose_embedder.GeneralPurposeEmbedder.embed_query')
        # mock_embedder_run -> from @patch('scripts.embedding.general_purpose_embedder.GeneralPurposeEmbedder.run')
        # mock_text_chunker_chunk_text_call -> from @patch('scripts.chunking.text_chunker_v2.TextChunker.chunk_text')
        # The new mock for chunk_retriever_v3.APIClient needs to be added.
        # If added as the last decorator, it's the first argument.

        # Let's re-declare the signature to match the decorator stack (applied bottom-up for arguments)
        # Original decorators:
        # @patch('scripts.chunking.text_chunker_v2.TextChunker.__init__', MagicMock(return_value=None)) (A)
        # @patch('scripts.chunking.text_chunker_v2.TextChunker.chunk_text', side_effect=mock_chunk_text) (B)
        # @patch('scripts.embedding.general_purpose_embedder.GeneralPurposeEmbedder.__init__', MagicMock(return_value=None)) (C)
        # @patch('scripts.embedding.general_purpose_embedder.GeneralPurposeEmbedder.run') (D)
        # @patch('scripts.embedding.general_purpose_embedder.GeneralPurposeEmbedder.embed_query') (E)
        # @patch('scripts.api_clients.openai.gptApiClient.APIClient.send_completion_request') (F)
        # NEW: @patch('scripts.retrieval.chunk_retriever_v3.APIClient') (G)
        # Arguments should be G, F, E, D, B
        
        # Correct mock setup for clarity:
        # mock_send_completion_request is the one for generate_answer.
        # mock_chunk_retriever_api_client is for the APIClient in ChunkRetriever.
        mock_chunk_retriever_api_client.return_value = MagicMock() # Prevent init error
        mock_generate_answer_api_client.return_value = "This is a mock LLM answer based on text files." # This is actually patching the class, not method.
                                                                                                   # The original patch for send_completion_request was better.

        # Sticking to the original patch for send_completion_request and adding one for APIClient in chunk_retriever
        # The new @patch for scripts.retrieval.chunk_retriever_v3.APIClient is added.
        # Its corresponding arg will be the first in the list.
        # The mock for APIClient.send_completion_request used by generate_answer needs to be set on its return_value if it's a class mock
        # or directly if it's a method mock. The current @patch is on the method.
        
        # Let's simplify. The original mock_send_completion_request is for the method.
        # The new mock_chunk_retriever_api_client is for the class used in ChunkRetriever.
        
        # mock_chunk_retriever_api_client is the first argument from the new decorator.
        # mock_generate_answer_api_client is for the send_completion_request method.

        mock_chunk_retriever_api_client.return_value = MagicMock() # Prevents __init__ error
        # The actual mock for send_completion_request is the other argument.
        # Let's rename them in the signature for clarity.
        # Original order of args: mock_send_completion_request, mock_embed_query, mock_embedder_run, mock_text_chunker_chunk_text_call
        # New order with mock_chunk_retriever_api_client as first arg from its decorator:
        # def test_text_file_processing_workflow(self, mock_api_client_in_chunk_retriever, mock_send_completion_req_in_generate_answer, ...)
        
        # Using the names from the current signature, and assuming the new patch is the last one applied (so its arg comes first)
        # mock_chunk_retriever_api_client is the new first arg.
        # mock_send_completion_request is now the second arg.
        
        # This means:
        # mock_chunk_retriever_api_client is from @patch('scripts.retrieval.chunk_retriever_v3.APIClient')
        # mock_generate_answer_api_client is from @patch('scripts.api_clients.openai.gptApiClient.APIClient.send_completion_request')
        
        mock_chunk_retriever_api_client.return_value = MagicMock() # To handle __init__ in ChunkRetriever
        mock_generate_answer_api_client.return_value = "This is a mock LLM answer based on text files." # For send_completion_request in RAGPipeline.generate_answer

        # 1. Extract and Chunk
        # Mock for embedder.run:
        # Simulate creation of index and metadata files
        def simulate_embedder_run(chunk_file_path, text_column):
            # chunk_file_path is self.pipeline.chunked_file
            # text_column is "Chunk"
            # We need to access config from self.pipeline as it's not passed directly
            embedding_output_dir = Path(self.pipeline.config["embedding"]["output_dir"])
            index_filename = self.pipeline.config["embedding"]["index_filename"]
            metadata_filename = self.pipeline.config["embedding"]["metadata_filename"]
            
            embedding_output_dir.mkdir(parents=True, exist_ok=True) # Ensure dir exists
            (embedding_output_dir / index_filename).touch() # Dummy FAISS index
            
            # Create dummy metadata file with headers and one row
            metadata_path = embedding_output_dir / metadata_filename
            # Based on GeneralPurposeEmbedder, it takes df_chunks and adds an 'id' column.
            # df_chunks (from extract_and_chunk) has: "File Path", "Raw Text", "Cleaned Text", "Chunk"
            # So, metadata will have: "id", "File Path", "Raw Text", "Cleaned Text", "Chunk"
            # ChunkRetriever uses 'id' and 'Chunk' (text_column).
            dummy_metadata_df = pd.DataFrame({
                'id': ['mock_chunk_id_0', 'mock_chunk_id_1', 'mock_chunk_id_2', 'mock_chunk_id_3'], # Matching 4 chunks from mock_chunk_text
                'File Path': ['/tmp/doc1.txt', '/tmp/doc1.txt', '/tmp/doc2.txt', '/tmp/doc2.txt'], # Example paths
                'Raw Text': [
                    "The quick brown fox jumps over the lazy dog.", 
                    "The quick brown fox jumps over the lazy dog.",
                    "Pack my box with five dozen liquor jugs.",
                    "Pack my box with five dozen liquor jugs."
                ],
                'Cleaned Text': [ # Assuming simple strip for cleaned
                    "The quick brown fox jumps over the lazy dog.",
                    "The quick brown fox jumps over the lazy dog.",
                    "Pack my box with five dozen liquor jugs.",
                    "Pack my box with five dozen liquor jugs."
                ],
                text_column: [ # text_column is 'Chunk'
                    "The quick brown fox", "jumps over the lazy dog.",
                    "Pack my box with five", "dozen liquor jugs."
                ]
            })
            dummy_metadata_df.to_csv(metadata_path, sep='\t', index=False)
            return True

        mock_embedder_run.side_effect = simulate_embedder_run
        
        # Mock for embedder.embed_query:
        # Return a dummy vector of the correct dimension (from config)
        dummy_vector = [0.1] * self.config_data["embedding"]["embedding_dim"]
        mock_embed_query.return_value = dummy_vector

        # 1. Extract and Chunk
        chunked_file_path_returned = self.pipeline.extract_and_chunk(data_type="text_file")
        
        self.assertEqual(self.pipeline.data_type, "text_file")
        
        expected_chunked_file_path = self.config_data["paths"]["chunked_text_files"]
        self.assertEqual(Path(chunked_file_path_returned).resolve(), Path(expected_chunked_file_path).resolve())
        self.assertTrue(Path(chunked_file_path_returned).exists())

        chunked_df = pd.read_csv(chunked_file_path_returned, sep='\t')
        # With mocked chunking: "The quick brown fox jumps over the lazy dog." -> 2 chunks
        # "Pack my box with five dozen liquor jugs." -> 2 chunks
        # Total 4 chunks
        self.assertEqual(len(chunked_df), 4) 
        self.assertIn("File Path", chunked_df.columns)
        self.assertIn("Chunk", chunked_df.columns) 
        self.assertIn("Raw Text", chunked_df.columns)
        self.assertIn(self.file1_content, chunked_df["Raw Text"].values) # Raw text should still be original
        self.assertIn(self.file2_content, chunked_df["Raw Text"].values)
        self.assertIn("The quick brown fox", chunked_df["Chunk"].values) # Check for a mocked chunk
        self.assertIn("dozen liquor jugs.", chunked_df["Chunk"].values) # Check for another mocked chunk


        # 2. Embed Chunks
        self.pipeline.embed_chunks() # This will now use the mocked GeneralPurposeEmbedder.run
        
        embedding_output_dir = Path(self.config_data["embedding"]["output_dir"])
        index_file = embedding_output_dir / self.config_data["embedding"]["index_filename"]
        metadata_file = embedding_output_dir / self.config_data["embedding"]["metadata_filename"]
        
        self.assertTrue(index_file.exists(), f"Mocked FAISS index file not found at {index_file}")
        self.assertTrue(metadata_file.exists(), f"Mocked Metadata file not found at {metadata_file}")
        mock_embedder_run.assert_called_once_with(chunked_file_path_returned, text_column="Chunk")


        # 3. Get User Query
        test_query = "What jumps over the lazy dog?"
        self.pipeline.get_user_query(test_query)
        self.assertEqual(self.pipeline.query, test_query)

        # 4. Retrieve
        retrieval_result = self.pipeline.retrieve() # Uses mocked embed_query
        self.assertIsInstance(retrieval_result, dict)
        self.assertIn("context", retrieval_result)
        self.assertIn("top_chunks", retrieval_result)
        # With mocked retriever (if not deeply mocking ChunkRetriever itself), context might be empty or based on dummy files.
        # For now, we assume ChunkRetriever can handle dummy index/metadata if it reads them.
        # If ChunkRetriever itself needs more mocking, this part might need adjustment.
        # Given the current setup, it will try to read the dummy files. It might fail if they are empty.
        # Let's assume for now it "runs". A more robust test would mock ChunkRetriever.retrieve
        mock_embed_query.assert_called_once_with(test_query)


        # 5. Generate Answer
        generated_answer = self.pipeline.generate_answer() # Uses mocked send_completion_request
        
        self.assertEqual(generated_answer, "This is a mock LLM answer based on text files.")
        mock_send_completion_request.assert_called_once()
        
        # Check that TextFilePromptBuilder was used. This is implicitly tested by the fact that
        # self.pipeline.data_type was "text_file", and generate_answer uses the correct builder.
        # We can also check the generated run directory for generate_answer
        # Find the latest run directory in the task's runs_dir
        task_runs_dir = Path(self.pipeline_runs_base_dir) / self.task_name / "runs"
        self.assertTrue(task_runs_dir.exists())
        
        all_run_ids = [d.name for d in task_runs_dir.iterdir() if d.is_dir()]
        self.assertTrue(len(all_run_ids) > 0)
        latest_run_id = sorted(all_run_ids)[-1]
        
        answer_output_dir = task_runs_dir / latest_run_id
        self.assertTrue((answer_output_dir / "answer.txt").exists())
        self.assertTrue((answer_output_dir / "query_debug.txt").exists())
        self.assertTrue((answer_output_dir / "run_metadata.json").exists())

if __name__ == '__main__':
    # This allows running the test directly if needed, e.g. python -m scripts.pipeline.test_rag_pipeline_text_files
    # However, typically tests are run via `python -m unittest discover -s scripts` or similar
    unittest.main()
